{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "실험1 최적루트\n"
      ],
      "metadata": {
        "id": "Tj95637EXpHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NSZM_wRBThHi"
      },
      "outputs": [],
      "source": [
        "## frozen-lake 문제에 대한 DQN 프로그램.\n",
        "##\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "total_episodes = 60000 # Total number of episodes in training\n",
        "max_steps = 99 # Max steps per episode in training.\n",
        "gamma = 0.90 # Discounting rate for expected return\n",
        "Learning_rate = 0.00005 # 신경망 모델 learning rate (optimizer 에게 제공)\n",
        "original_epsilon = 0.4 # Exploration rate\n",
        "decay_rate = 0.000006 # Exponential decay rate for exploration.\n",
        "TAU = 0.7 # Q_net 파라메터를 Q_hat_net 로 copy 시에 반영 비율.\n",
        "one_minus_TAU = 1 - TAU\n",
        "\n",
        "memory_pos = 0 # replay_memory 내에 transition 을 넣을 다음 위치.\n",
        "# 0 에서 부터 커지다가 max_memory-1 까지 되면 다시 0 부터 시작함.\n",
        "BATCH_SIZE = 16\n",
        "model_update_cnt = 0 # Q_net 를 업데이트한 횟수.\n",
        "copy_cnt = 4 # Q_net 업데이트를 copy_cnt 번 한 후마다 Q_hat_net 로 파라메터 복사.\n",
        "max_memory = 2000 # capacity of the replay memory.\n",
        "transition_cnt = 0 # 거쳐간 총 transition 수(episodes 간에 중단 없이)\n",
        "# 매 (배치크기+작은 랜덤값) 마다 Q_net 의 parameter update 를 수행함.\n",
        "random.seed(datetime.now().timestamp()) # give a new seed in random number generation.\n",
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "max_row = 9\n",
        "max_col = 9\n",
        "n_actions = 4 # 0:up, 1:right, 2:down, 3:left.\n",
        "n_observations = max_row * max_col # total number of states\n",
        "# 1-hot 벡터로 표현하므로 NN 입력의 신호수 = 총 state 수\n",
        "env_state_space = [\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'],\\\n",
        "    ['H', 'S', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'F', 'H', 'H', 'F', 'H', 'F', 'H'],\\\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'H', 'F', 'H', 'F', 'F', 'H', 'H'],\\\n",
        "    ['H', 'F', 'F', 'F', 'F', 'G', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'H', 'H', 'F', 'H', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\\\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
        "]\n",
        "# offset of each move action: up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "move_offset = [[-1,0], [0,1], [1,0], [0,-1]]\n",
        "move_str = ['up ', 'right', 'down ', 'left ']\n",
        "# replay memory: transition 들을 저장하는 버퍼.\n",
        "# 저장되는 transition 의 4가지 정보: state_index, action, reward, next state index.\n",
        "# (주의: state 를 좌표 대신 상태번호(index) 로 나타냄.)\n",
        "replay_memory = np.ndarray((max_memory, 4), dtype=int)\n",
        "batch_transition = np.ndarray((BATCH_SIZE, 4), dtype=int) # 배치 하나를 넣는데 사용.\n",
        "is_replay_memory_full = 0 # 버퍼가 처음으로 완전히 채워지기 전에는 0. 그후로는 항상 1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_and_print_Q_values (s):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    if env_state_space[r][c] == 'G' or env_state_space[r][c] == 'H':\n",
        "        action_values = [ 0.0 for i in range(n_actions)]\n",
        "        action_values = np.array(action_values)\n",
        "    else:\n",
        "        state_idx = r * max_col + c # state 의 번호를 만듬.\n",
        "        state_idx_list = [state_idx] # 배치 차원을 넣는다. 배치는 하나의 예제 입력만 가짐.\n",
        "        states_tsr = torch.tensor(state_idx_list).to(device) # state 한개 가짐\n",
        "        one_hot_states_tsr = F.one_hot(states_tsr, num_classes= n_observations)\n",
        "        one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "        with torch.no_grad():\n",
        "            state_action_values = Q_net(one_hot_states_tsr) # 주의: 출력은 2차원: (1, n_actions).\n",
        "        state_action_values = state_action_values[0] # 배치 차원을 없앤다.\n",
        "        action_values = state_action_values.cpu().numpy()\n",
        "    text = \"s[\" + str(r) + \",\" + str(c) + \"]: \"\n",
        "    for i in range(n_actions):\n",
        "    #text = text + str(action_values[i]) + \", \"\n",
        "        text = text + \"{:5.2f}\".format(action_values[i])+ \", \"\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "jzE4ny69T43i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_argmax(q_a_list):\n",
        "    max_val = np.max(q_a_list)\n",
        "    max_positions = np.where(q_a_list == max_val)[0]\n",
        "    nmax = len(max_positions)\n",
        "\n",
        "    if nmax == 1:\n",
        "        return max_positions[0]  # 하나의 최대값이 있다면 그 위치를 반환\n",
        "\n",
        "    # 최대값이 여러 개인 경우, 랜덤하게 하나의 위치를 선택하여 반환\n",
        "    return torch.tensor(random.choice(max_positions))"
      ],
      "metadata": {
        "id": "MdWF4uUPFcu5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def choose_action_with_greedy(s):\n",
        "    state_idx = s[0] * max_col + s[1]  # 상태를 번호로 변환합니다.\n",
        "    state_idx_list = [state_idx]\n",
        "    states_tsr = torch.tensor(state_idx_list).to(device)  # 하나의 상태를 포함하는 배열\n",
        "    one_hot_states_tsr = F.one_hot(states_tsr, num_classes=n_observations)\n",
        "    one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "\n",
        "    # 상태 s에 대한 모든 액션의 q-a 값 가져오기\n",
        "    with torch.no_grad():\n",
        "        state_action_values = Q_net(one_hot_states_tsr)  # 출력은 2차원: (1, n_actions).\n",
        "\n",
        "    lst=state_action_values.flatten().tolist()\n",
        "    max_a = my_argmax(lst) # 최대값 중에서 무작위로 선택하기\n",
        "\n",
        "    return max_a\n",
        "\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "    state_idx = s[0] * max_col + s[1]  # 상태를 번호로 변환합니다.\n",
        "    state_idx_list = [state_idx]  # 배치 차원 추가\n",
        "    states_tsr = torch.tensor(state_idx_list).to(device)  # 하나의 상태를 가집니다.\n",
        "    one_hot_states_tsr = F.one_hot(states_tsr, num_classes=n_observations)\n",
        "    one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "\n",
        "    # 상태 s에 대한 모든 액션의 q-a 값 가져오기\n",
        "    with torch.no_grad():\n",
        "        state_action_values = Q_net(one_hot_states_tsr)  # 출력은 2차원: (bsz, n_actions).\n",
        "\n",
        "    lst=state_action_values.flatten().tolist()\n",
        "    max_a = my_argmax(lst) # 최대값 중에서 무작위로 선택하기\n",
        "\n",
        "    rn = random.random()  # 0 ~ 1 사이의 난수 생성\n",
        "    if rn >= epsilon:  # epsilon 이상인 경우, 최대 확률을 가진 액션 선택\n",
        "        action = max_a\n",
        "    else:\n",
        "        rn1 = random.random()\n",
        "        # 4개의 액션 중 하나를 무작위로 선택\n",
        "        if rn1 >= 0.75:\n",
        "            action = 0\n",
        "        elif rn1 >= 0.5:\n",
        "            action = 1\n",
        "        elif rn1 >= 0.25:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "\n",
        "    return action\n",
        "\n",
        "def get_new_state_and_reward(s, a):\n",
        "\tnew_state = []\n",
        "\toff_set = move_offset[a]\n",
        "\n",
        "\t#  s + off_set gives the new_state.\n",
        "\tnew_state.append(s[0] + off_set[0])\n",
        "\tnew_state.append(s[1] + off_set[1])\n",
        "\n",
        "\t# compute reward for moving to the new state\n",
        "\tcell = env_state_space[new_state[0]][new_state[1]]\n",
        "\tif cell == 'F':\n",
        "\t\trew = 0\n",
        "\telif cell == 'H':\n",
        "\t\trew = -9\n",
        "\telif cell == 'G':\n",
        "\t\trew = 9\n",
        "\telif cell == 'S':\n",
        "\t\trew = 0\n",
        "\telse:\n",
        "\t\tprint(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "\t\ttime.sleep(1200)\n",
        "\t\treturn [0,0], -20000\n",
        "\treturn new_state, rew"
      ],
      "metadata": {
        "id": "i2tcAT4HUff9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        # 신경망의 각 레이어 정의\n",
        "        self.layer1 = nn.Linear(n_observations, 128)  # 입력 뉴런 수: n_observations, 출력 뉴런 수: 128\n",
        "        self.layer2 = nn.Linear(128, 128)  # 입력 뉴런 수: 128, 출력 뉴런 수: 128\n",
        "        self.layer3 = nn.Linear(128, n_actions)  # 입력 뉴런 수: 128, 출력 뉴런 수: n_actions\n",
        "\n",
        "    # forward 함수: 데이터가 모델을 통과할 때 호출되는 함수\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))  # 첫 번째 레이어에 ReLU 활성화 함수를 적용\n",
        "        x = F.relu(self.layer2(x))  # 두 번째 레이어에 ReLU 활성화 함수를 적용\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "4XTAxddpVFF1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_by_a_batch():\n",
        "    state_batch=batch_transition[:,0]\n",
        "    action_batch=batch_transition[:,1]\n",
        "    reward_batch=batch_transition[:,2]\n",
        "    next_state_batch=batch_transition[:,3]\n",
        "\n",
        "    state_batch_tsr=torch.from_numpy(state_batch)\n",
        "    one_hot_state_batch=F.one_hot(state_batch_tsr,num_classes=n_observations)\n",
        "    one_hot_state_batch=one_hot_state_batch.float().to(device)\n",
        "\n",
        "    prediction_Q_net=Q_net(one_hot_state_batch)\n",
        "\n",
        "    state_action_value_tsr=torch.zeros([BATCH_SIZE,],dtype=torch.float64).to(device)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        state_action_value_tsr[i]=prediction_Q_net[i,action_batch[i]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_state_batch_tsr=torch.from_numpy(next_state_batch)\n",
        "        one_hot_next_state_batch=F.one_hot(next_state_batch_tsr,num_classes=n_observations)\n",
        "        one_hot_next_state_batch=one_hot_next_state_batch.float().to(device)\n",
        "        result_target_net=Q_hat_net(one_hot_next_state_batch)\n",
        "\n",
        "        max_q_of_next_states_in_batch=torch.max(result_target_net,dim=1).values\n",
        "\n",
        "    next_state_values=[]\n",
        "    for i,st in enumerate(next_state_batch):\n",
        "        r=int(st/max_col)\n",
        "        c=st%max_col\n",
        "        if env_state_space[r][c]=='G' or env_state_space[r][c]=='H':\n",
        "            next_state_values.append(0)\n",
        "        else:\n",
        "            next_state_values.append(max_q_of_next_states_in_batch[i].item())\n",
        "\n",
        "    next_state_values_tsr=torch.tensor(next_state_values).to(device)\n",
        "    reward_batch_tsr=torch.from_numpy(reward_batch).to(device)\n",
        "    target_state_action_values_tsr=(next_state_values_tsr*gamma)+reward_batch_tsr\n",
        "\n",
        "    loss=criterion(state_action_value_tsr,target_state_action_values_tsr)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(Q_net.parameters(),100)\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "Q_net=DQN(n_observations,n_actions).to(device)\n",
        "Q_hat_net=DQN(n_observations,n_actions).to(device)\n",
        "\n",
        "Q_hat_net.load_state_dict(Q_net.state_dict())\n",
        "\n",
        "optimizer=optim.AdamW(Q_net.parameters(),lr=Learning_rate,amsgrad=True)\n",
        "criterion=nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "fvSdk3akVXRs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episode=total_episodes\n",
        "start_state=[1,1]\n",
        "print(\"\\n학습시작\\n\")\n",
        "\n",
        "for i_episode in range(num_episode):\n",
        "    S=start_state\n",
        "    epsilon=original_epsilon*math.exp(-decay_rate*i_episode)\n",
        "\n",
        "    if i_episode != 0 and i_episode %4000==0:\n",
        "          print('episode=',i_episode,'  epsilon=',epsilon)\n",
        "\n",
        "    for  t in range(max_steps):\n",
        "        A=choose_action_with_epsilon_greedy(S,epsilon)\n",
        "        S_,R = get_new_state_and_reward(S,A)\n",
        "\n",
        "        s_idx=S[0]*max_col+S[1]\n",
        "        next_s_idx=S_[0]*max_col+S_[1]\n",
        "\n",
        "        replay_memory[memory_pos,0]= s_idx\n",
        "        replay_memory[memory_pos,1]=A\n",
        "        replay_memory[memory_pos,2]=R\n",
        "        replay_memory[memory_pos,3]=next_s_idx\n",
        "\n",
        "        if is_replay_memory_full==0 and memory_pos==max_memory-1:\n",
        "            is_replay_memory_full=1\n",
        "        memory_pos=(memory_pos+1) % max_memory\n",
        "        S=S_\n",
        "        transition_cnt+=1\n",
        "        random_number=random.randint(0,int(BATCH_SIZE/2))\n",
        "\n",
        "        if transition_cnt >= (BATCH_SIZE+3) and transition_cnt % (BATCH_SIZE+random_number):\n",
        "            if is_replay_memory_full==1:\n",
        "                random_numbers=random.sample(range(0,max_memory),BATCH_SIZE)\n",
        "            else:\n",
        "                random_numbers=random.sample(range(0,memory_pos-1),BATCH_SIZE)\n",
        "\n",
        "            for i in range(BATCH_SIZE):\n",
        "                rnum=random_numbers[i]\n",
        "                batch_transition[i,:]=replay_memory[rnum,:]\n",
        "\n",
        "            learning_by_a_batch()\n",
        "            model_update_cnt+=1\n",
        "\n",
        "            if model_update_cnt % copy_cnt==0:\n",
        "                Q_hat_net_state_dict=Q_hat_net.state_dict()\n",
        "                Q_net_state_dict=Q_net.state_dict()\n",
        "                for key in Q_net_state_dict:\n",
        "                    Q_hat_net_state_dict[key]=Q_net_state_dict[key]*TAU+Q_net_state_dict[key]*one_minus_TAU\n",
        "                Q_hat_net.load_state_dict(Q_hat_net_state_dict)\n",
        "        if env_state_space[S[0]][S[1]]=='G' or env_state_space[S[0]][S[1]]=='H':\n",
        "            break\n",
        "\n",
        "print('학습종료\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06fZsSuAFVMO",
        "outputId": "15f33c4b-4915-478b-a30a-490fcc75cf42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "학습시작\n",
            "\n",
            "episode= 4000   epsilon= 0.39051428390316373\n",
            "episode= 8000   epsilon= 0.3812535148310019\n",
            "episode= 12000   epsilon= 0.3722123583244823\n",
            "episode= 16000   epsilon= 0.3633856064274825\n",
            "episode= 20000   epsilon= 0.354768174686863\n",
            "episode= 24000   epsilon= 0.346355099223682\n",
            "episode= 28000   epsilon= 0.33814153387386353\n",
            "episode= 32000   epsilon= 0.33012274739667297\n",
            "episode= 36000   epsilon= 0.3222941207493919\n",
            "episode= 40000   epsilon= 0.31465114442662134\n",
            "episode= 44000   epsilon= 0.30718941586268245\n",
            "episode= 48000   epsilon= 0.2999046368956165\n",
            "episode= 52000   epsilon= 0.29279261129132506\n",
            "episode= 56000   epsilon= 0.2858492423264229\n",
            "학습종료\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('테스트 시작')\n",
        "\n",
        "cnt=0\n",
        "for e in range(100):\n",
        "    S=start_state\n",
        "    total_reward=0\n",
        "\n",
        "    leng=0\n",
        "    for i in range(99):\n",
        "        A=choose_action_with_greedy(S)\n",
        "        S_,R=get_new_state_and_reward(S,A)\n",
        "        leng+=1\n",
        "        total_reward+=R\n",
        "        S=S_\n",
        "        if env_state_space[S[0]][S[1]]=='G' or env_state_space[S[0]][S[1]]=='H':\n",
        "            break\n",
        "\n",
        "    if total_reward==9 and leng==8:\n",
        "        cnt+=1\n",
        "print(cnt)\n",
        "print(f'the optinal accuracy: {cnt}%')\n",
        "print(\"테스트 종료\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObACdrfObsCd",
        "outputId": "c0edbed4-6917-4f7c-a37f-ce256a5bd938"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 시작\n",
            "100\n",
            "the optinal accuracy: 100%\n",
            "테스트 종료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('학습 후의 Q_values:')\n",
        "for i in range(max_row):\n",
        "    for j in range(max_col):\n",
        "        s=[i,j]\n",
        "        compute_and_print_Q_values(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VyBjweGpXtz",
        "outputId": "bc57fe67-69dc-4860-ca2d-e9969c455a79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 후의 Q_values:\n",
            "s[0,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,1]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[1,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[1,1]: -9.00,  4.30,  4.31, -9.00, \n",
            "s[1,2]: -9.00,  4.78,  4.78,  3.88, \n",
            "s[1,3]: -9.00,  5.31, -9.01,  4.30, \n",
            "s[1,4]: -9.00,  5.90, -9.02,  4.78, \n",
            "s[1,5]: -8.99,  5.31,  6.56,  5.31, \n",
            "s[1,6]: -8.96,  4.81, -8.95,  5.90, \n",
            "s[1,7]:  0.40, -5.51,  5.37,  1.20, \n",
            "s[1,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,1]:  3.87,  4.78,  4.79, -9.00, \n",
            "s[2,2]:  4.31, -8.99,  5.32,  4.30, \n",
            "s[2,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,5]:  5.90, -8.99,  7.29, -9.00, \n",
            "s[2,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,7]:  3.72, -5.09,  5.86, -7.71, \n",
            "s[2,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,1]:  4.30,  5.31,  5.31, -9.00, \n",
            "s[3,2]:  4.78,  5.90, -9.01,  4.78, \n",
            "s[3,3]: -9.00,  6.56,  6.54,  5.32, \n",
            "s[3,4]: -9.00,  7.29, -9.01,  5.90, \n",
            "s[3,5]:  6.56,  6.56,  8.10,  6.56, \n",
            "s[3,6]: -8.98,  5.90,  7.26,  7.29, \n",
            "s[3,7]:  4.72, -6.99, -7.27,  6.60, \n",
            "s[3,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,1]:  4.78, -8.96,  5.89, -8.99, \n",
            "s[4,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,3]:  5.91, -8.94,  7.27, -8.99, \n",
            "s[4,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,5]:  7.29,  7.28,  9.01, -8.99, \n",
            "s[4,6]:  6.57, -8.97,  7.97,  8.10, \n",
            "s[4,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,1]:  5.30,  6.56,  5.30, -8.95, \n",
            "s[5,2]: -8.99,  7.29, -8.99,  5.88, \n",
            "s[5,3]:  6.53,  8.10, -9.01,  6.54, \n",
            "s[5,4]: -8.97,  9.00,  7.24,  7.28, \n",
            "s[5,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,6]:  4.08,  4.60,  6.15,  8.91, \n",
            "s[5,7]: -0.09,  0.54,  6.09,  0.25, \n",
            "s[5,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,1]:  5.89, -8.79,  4.72, -8.93, \n",
            "s[6,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,4]:  8.08, -8.91,  6.24, -8.90, \n",
            "s[6,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,6]:  7.77,  0.75,  5.70, -7.54, \n",
            "s[6,7]:  2.02, -4.95, -0.18,  6.85, \n",
            "s[6,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[7,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[7,1]:  5.27,  1.20, -3.61, -5.85, \n",
            "s[7,2]: -8.45,  1.80, -3.31,  4.44, \n",
            "s[7,3]: -4.18,  1.36, -1.67,  3.73, \n",
            "s[7,4]:  6.92,  5.15, -4.65,  3.02, \n",
            "s[7,5]: -7.17,  6.12, -4.38,  5.00, \n",
            "s[7,6]:  6.96,  0.66, -5.48,  1.96, \n",
            "s[7,7]:  1.14, -0.22, -1.47,  2.43, \n",
            "s[7,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,1]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,8]:  0.00,  0.00,  0.00,  0.00, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실험 2 캡처 & 리플레이 없을 때의 성능"
      ],
      "metadata": {
        "id": "it4-ypUbooiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## frozen-lake 문제에 대한 DQN 프로그램.\n",
        "##\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "total_episodes = 60000 # Total number of episodes in training\n",
        "max_steps = 99 # Max steps per episode in training.\n",
        "gamma = 0.90 # Discounting rate for expected return\n",
        "Learning_rate = 0.00005 # 신경망 모델 learning rate (optimizer 에게 제공)\n",
        "original_epsilon = 0.4 # Exploration rate\n",
        "decay_rate = 0.000006 # Exponential decay rate for exploration.\n",
        "TAU = 0.7 # Q_net 파라메터를 Q_hat_net 로 copy 시에 반영 비율.\n",
        "one_minus_TAU = 1 - TAU\n",
        "\n",
        "memory_pos = 0 # replay_memory 내에 transition 을 넣을 다음 위치.\n",
        "# 0 에서 부터 커지다가 max_memory-1 까지 되면 다시 0 부터 시작함.\n",
        "BATCH_SIZE = 16\n",
        "model_update_cnt = 0 # Q_net 를 업데이트한 횟수.\n",
        "copy_cnt = 4 # Q_net 업데이트를 copy_cnt 번 한 후마다 Q_hat_net 로 파라메터 복사.\n",
        "max_memory = BATCH_SIZE # capacity of the replay memory.\n",
        "transition_cnt = 0 # 거쳐간 총 transition 수(episodes 간에 중단 없이)\n",
        "# 매 (배치크기+작은 랜덤값) 마다 Q_net 의 parameter update 를 수행함.\n",
        "random.seed(datetime.now().timestamp()) # give a new seed in random number generation.\n",
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "max_row = 9\n",
        "max_col = 9\n",
        "n_actions = 4 # 0:up, 1:right, 2:down, 3:left.\n",
        "n_observations = max_row * max_col # total number of states\n",
        "# 1-hot 벡터로 표현하므로 NN 입력의 신호수 = 총 state 수\n",
        "env_state_space = [\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'],\\\n",
        "    ['H', 'S', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'F', 'H', 'H', 'F', 'H', 'F', 'H'],\\\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'H', 'F', 'H', 'F', 'F', 'H', 'H'],\\\n",
        "    ['H', 'F', 'F', 'F', 'F', 'G', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'H', 'H', 'F', 'H', 'F', 'F', 'H'],\\\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\\\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
        "]\n",
        "# offset of each move action: up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "move_offset = [[-1,0], [0,1], [1,0], [0,-1]]\n",
        "move_str = ['up ', 'right', 'down ', 'left ']\n",
        "# replay memory: transition 들을 저장하는 버퍼.\n",
        "# 저장되는 transition 의 4가지 정보: state_index, action, reward, next state index.\n",
        "# (주의: state 를 좌표 대신 상태번호(index) 로 나타냄.)\n",
        "replay_memory = np.ndarray((max_memory, 4), dtype=int)\n",
        "batch_transition = np.ndarray((BATCH_SIZE, 4), dtype=int) # 배치 하나를 넣는데 사용.\n",
        "is_replay_memory_full = 0 # 버퍼가 처음으로 완전히 채워지기 전에는 0. 그후로는 항상 1."
      ],
      "metadata": {
        "id": "QA8ecPVSo0h4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_argmax(q_a_list):\n",
        "    max_val = np.max(q_a_list)\n",
        "    max_positions = np.where(q_a_list == max_val)[0]\n",
        "    nmax = len(max_positions)\n",
        "\n",
        "    if nmax == 1:\n",
        "        return max_positions[0]  # 하나의 최대값이 있다면 그 위치를 반환\n",
        "\n",
        "    # 최대값이 여러 개인 경우, 랜덤하게 하나의 위치를 선택하여 반환\n",
        "    return torch.tensor(random.choice(max_positions))"
      ],
      "metadata": {
        "id": "orID5Zr3Bwgs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my_argmax 실험\n",
        "state_action_values = torch.tensor([[3.2, 5.1, 2.8, 5.1]])\n",
        "max_a = torch.argmax(state_action_values, dim=1)\n",
        "lst=state_action_values.flatten().tolist()\n",
        "rmax=my_argmax(lst)\n",
        "max_a = max_a[0]\n",
        "print(rmax)\n",
        "print(max_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSOvyAO72XED",
        "outputId": "b1d416aa-02f6-416e-83fe-0b0341dbd4af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n",
            "tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def choose_action_with_greedy(s):\n",
        "    state_idx = s[0] * max_col + s[1]  # 상태를 번호로 변환합니다.\n",
        "    state_idx_list = [state_idx]\n",
        "    states_tsr = torch.tensor(state_idx_list).to(device)  # 하나의 상태를 포함하는 배열\n",
        "    one_hot_states_tsr = F.one_hot(states_tsr, num_classes=n_observations)\n",
        "    one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "\n",
        "    # 상태 s에 대한 모든 액션의 q-a 값 가져오기\n",
        "    with torch.no_grad():\n",
        "        state_action_values = Q_net(one_hot_states_tsr)  # 출력은 2차원: (1, n_actions).\n",
        "\n",
        "    lst=state_action_values.flatten().tolist()\n",
        "    max_a = my_argmax(lst) # 최대값 중에서 무작위로 선택하기\n",
        "\n",
        "    return max_a\n",
        "\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "    state_idx = s[0] * max_col + s[1]  # 상태를 번호로 변환합니다.\n",
        "    state_idx_list = [state_idx]  # 배치 차원 추가\n",
        "    states_tsr = torch.tensor(state_idx_list).to(device)  # 하나의 상태를 가집니다.\n",
        "    one_hot_states_tsr = F.one_hot(states_tsr, num_classes=n_observations)\n",
        "    one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "\n",
        "    # 상태 s에 대한 모든 액션의 q-a 값 가져오기\n",
        "    with torch.no_grad():\n",
        "        state_action_values = Q_net(one_hot_states_tsr)  # 출력은 2차원: (bsz, n_actions).\n",
        "\n",
        "    lst=state_action_values.flatten().tolist()\n",
        "    max_a = my_argmax(lst) # 최대값 중에서 무작위로 선택하기\n",
        "\n",
        "    rn = random.random()  # 0 ~ 1 사이의 난수 생성\n",
        "    if rn >= epsilon:  # epsilon 이상인 경우, 최대 확률을 가진 액션 선택\n",
        "        action = max_a\n",
        "    else:\n",
        "        rn1 = random.random()\n",
        "        # 4개의 액션 중 하나를 무작위로 선택\n",
        "        if rn1 >= 0.75:\n",
        "            action = 0\n",
        "        elif rn1 >= 0.5:\n",
        "            action = 1\n",
        "        elif rn1 >= 0.25:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "\n",
        "    return action\n",
        "\n",
        "def get_new_state_and_reward(s, a):\n",
        "\tnew_state = []\n",
        "\toff_set = move_offset[a]\n",
        "\n",
        "\t#  s + off_set gives the new_state.\n",
        "\tnew_state.append(s[0] + off_set[0])\n",
        "\tnew_state.append(s[1] + off_set[1])\n",
        "\n",
        "\t# compute reward for moving to the new state\n",
        "\tcell = env_state_space[new_state[0]][new_state[1]]\n",
        "\tif cell == 'F':\n",
        "\t\trew = 0\n",
        "\telif cell == 'H':\n",
        "\t\trew = -9\n",
        "\telif cell == 'G':\n",
        "\t\trew = 9\n",
        "\telif cell == 'S':\n",
        "\t\trew = 0\n",
        "\telse:\n",
        "\t\tprint(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "\t\ttime.sleep(1200)\n",
        "\t\treturn [0,0], -20000\n",
        "\treturn new_state, rew"
      ],
      "metadata": {
        "id": "gE2R9gmbo7kT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        # 신경망의 각 레이어 정의\n",
        "        self.layer1 = nn.Linear(n_observations, 128)  # 입력 뉴런 수: n_observations, 출력 뉴런 수: 128\n",
        "        self.layer2 = nn.Linear(128, 128)  # 입력 뉴런 수: 128, 출력 뉴런 수: 128\n",
        "        self.layer3 = nn.Linear(128, n_actions)  # 입력 뉴런 수: 128, 출력 뉴런 수: n_actions\n",
        "\n",
        "    # forward 함수: 데이터가 모델을 통과할 때 호출되는 함수\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))  # 첫 번째 레이어에 ReLU 활성화 함수를 적용\n",
        "        x = F.relu(self.layer2(x))  # 두 번째 레이어에 ReLU 활성화 함수를 적용\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "KY2H5pT_o_P_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_by_a_batch():\n",
        "    state_batch=batch_transition[:,0]\n",
        "    action_batch=batch_transition[:,1]\n",
        "    reward_batch=batch_transition[:,2]\n",
        "    next_state_batch=batch_transition[:,3]\n",
        "\n",
        "    state_batch_tsr=torch.from_numpy(state_batch)\n",
        "    one_hot_state_batch=F.one_hot(state_batch_tsr,num_classes=n_observations)\n",
        "    one_hot_state_batch=one_hot_state_batch.float().to(device)\n",
        "\n",
        "    prediction_Q_net=Q_net(one_hot_state_batch)\n",
        "\n",
        "    state_action_value_tsr=torch.zeros([BATCH_SIZE,],dtype=torch.float64).to(device)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        state_action_value_tsr[i]=prediction_Q_net[i,action_batch[i]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_state_batch_tsr=torch.from_numpy(next_state_batch)\n",
        "        one_hot_next_state_batch=F.one_hot(next_state_batch_tsr,num_classes=n_observations)\n",
        "        one_hot_next_state_batch=one_hot_next_state_batch.float().to(device)\n",
        "        result_target_net=Q_hat_net(one_hot_next_state_batch)\n",
        "\n",
        "        max_q_of_next_states_in_batch=torch.max(result_target_net,dim=1).values\n",
        "\n",
        "    next_state_values=[]\n",
        "    for i,st in enumerate(next_state_batch):\n",
        "        r=int(st/max_col)\n",
        "        c=st%max_col\n",
        "        if env_state_space[r][c]=='G' or env_state_space[r][c]=='H':\n",
        "            next_state_values.append(0)\n",
        "        else:\n",
        "            next_state_values.append(max_q_of_next_states_in_batch[i].item())\n",
        "\n",
        "    next_state_values_tsr=torch.tensor(next_state_values).to(device)\n",
        "    reward_batch_tsr=torch.from_numpy(reward_batch).to(device)\n",
        "    target_state_action_values_tsr=(next_state_values_tsr*gamma)+reward_batch_tsr\n",
        "\n",
        "    loss=criterion(state_action_value_tsr,target_state_action_values_tsr)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(Q_net.parameters(),100)\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "Q_net=DQN(n_observations,n_actions).to(device)\n",
        "Q_hat_net=DQN(n_observations,n_actions).to(device)\n",
        "\n",
        "Q_hat_net.load_state_dict(Q_net.state_dict())\n",
        "\n",
        "optimizer=optim.AdamW(Q_net.parameters(),lr=Learning_rate,amsgrad=True)\n",
        "criterion=nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "0mN9plaRpCdw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episode=total_episodes\n",
        "start_state=[1,1]\n",
        "print(\"\\n학습시작\\n\")\n",
        "\n",
        "for i_episode in range(num_episode):\n",
        "    S=start_state\n",
        "    epsilon=original_epsilon*math.exp(-decay_rate*i_episode)\n",
        "\n",
        "    if i_episode != 0 and i_episode %4000==0:\n",
        "          print('episode=',i_episode,'  epsilon=',epsilon)\n",
        "\n",
        "    for  t in range(max_steps):\n",
        "        A=choose_action_with_epsilon_greedy(S,epsilon)\n",
        "        S_,R = get_new_state_and_reward(S,A)\n",
        "\n",
        "        s_idx=S[0]*max_col+S[1]\n",
        "        next_s_idx=S_[0]*max_col+S_[1]\n",
        "\n",
        "        replay_memory[memory_pos,0]= s_idx\n",
        "        replay_memory[memory_pos,1]=A\n",
        "        replay_memory[memory_pos,2]=R\n",
        "        replay_memory[memory_pos,3]=next_s_idx\n",
        "\n",
        "        if is_replay_memory_full==0 and memory_pos==max_memory-1:\n",
        "            is_replay_memory_full=1\n",
        "        memory_pos=(memory_pos+1) % max_memory\n",
        "        S=S_\n",
        "        transition_cnt+=1\n",
        "        random_number=random.randint(0,int(BATCH_SIZE/2))\n",
        "\n",
        "        if is_replay_memory_full==1:\n",
        "            for i in range(BATCH_SIZE):\n",
        "                batch_transition[i,:]=replay_memory[i,:]\n",
        "\n",
        "            is_replay_memory_full=0\n",
        "\n",
        "            learning_by_a_batch()\n",
        "            model_update_cnt+=1\n",
        "\n",
        "            if model_update_cnt % copy_cnt==0:\n",
        "                Q_hat_net_state_dict=Q_hat_net.state_dict()\n",
        "                Q_net_state_dict=Q_net.state_dict()\n",
        "                for key in Q_net_state_dict:\n",
        "                    Q_hat_net_state_dict[key]=Q_net_state_dict[key]*TAU+Q_net_state_dict[key]*one_minus_TAU\n",
        "                Q_hat_net.load_state_dict(Q_hat_net_state_dict)\n",
        "        if env_state_space[S[0]][S[1]]=='G' or env_state_space[S[0]][S[1]]=='H':\n",
        "            break\n",
        "\n",
        "print('학습종료\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBLqYr2CpHMf",
        "outputId": "e20689c3-a8af-403b-885b-360a8b5df502"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "학습시작\n",
            "\n",
            "episode= 4000   epsilon= 0.39051428390316373\n",
            "episode= 8000   epsilon= 0.3812535148310019\n",
            "episode= 12000   epsilon= 0.3722123583244823\n",
            "episode= 16000   epsilon= 0.3633856064274825\n",
            "episode= 20000   epsilon= 0.354768174686863\n",
            "episode= 24000   epsilon= 0.346355099223682\n",
            "episode= 28000   epsilon= 0.33814153387386353\n",
            "episode= 32000   epsilon= 0.33012274739667297\n",
            "episode= 36000   epsilon= 0.3222941207493919\n",
            "episode= 40000   epsilon= 0.31465114442662134\n",
            "episode= 44000   epsilon= 0.30718941586268245\n",
            "episode= 48000   epsilon= 0.2999046368956165\n",
            "episode= 52000   epsilon= 0.29279261129132506\n",
            "episode= 56000   epsilon= 0.2858492423264229\n",
            "학습종료\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('테스트 시작')\n",
        "\n",
        "cnt=0\n",
        "for e in range(100):\n",
        "    S=start_state\n",
        "    total_reward=0\n",
        "    leng=0\n",
        "    for i in range(99):\n",
        "        A=choose_action_with_greedy(S)\n",
        "        S_,R=get_new_state_and_reward(S,A)\n",
        "        #print(f'move {move_str[A]} to {S_[0]},{S_[1]}')\n",
        "        leng+=1\n",
        "        total_reward+=R\n",
        "        S=S_\n",
        "        if env_state_space[S[0]][S[1]]=='G' or env_state_space[S[0]][S[1]]=='H':\n",
        "            break\n",
        "    if leng == 8 and total_reward==9 : cnt+=1\n",
        "    #print(f'ep{e}  lent={leng} R={total_reward}\\n')\n",
        "\n",
        "\n",
        "print(f'the optinal accuracy: {cnt}%')\n",
        "print(\"테스트 종료\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfir_oeopSO3",
        "outputId": "4d430b40-5d4b-47d9-ddb8-482de1a96c52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 시작\n",
            "the optinal accuracy: 100%\n",
            "테스트 종료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('학습 후의 Q_values:')\n",
        "for i in range(max_row):\n",
        "    for j in range(max_col):\n",
        "        s=[i,j]\n",
        "        compute_and_print_Q_values(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXyzV5Hlt9Ux",
        "outputId": "5a7b30f2-3cc4-47d0-e1fc-8b58ef1dc2e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 후의 Q_values:\n",
            "s[0,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,1]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[1,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[1,1]: -9.01,  3.62,  4.28, -9.02, \n",
            "s[1,2]: -8.43,  3.64,  4.02,  4.03, \n",
            "s[1,3]: -3.40,  3.02, -8.16,  3.75, \n",
            "s[1,4]: -2.95,  2.54, -7.86,  4.26, \n",
            "s[1,5]: -7.67,  2.58,  3.55,  4.70, \n",
            "s[1,6]: -3.65,  2.60, -8.08,  4.95, \n",
            "s[1,7]: -5.55, -7.00,  3.66,  2.95, \n",
            "s[1,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,1]:  3.86,  3.86,  4.76, -9.02, \n",
            "s[2,2]:  3.61, -8.99,  4.31,  4.04, \n",
            "s[2,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,5]:  3.34, -1.48,  3.04, -5.06, \n",
            "s[2,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,7]:  2.74, -2.72,  3.04, -4.64, \n",
            "s[2,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,1]:  4.29,  4.28,  5.29, -9.02, \n",
            "s[3,2]:  4.30,  4.01, -8.84,  4.77, \n",
            "s[3,3]: -7.48,  2.86,  2.36,  4.38, \n",
            "s[3,4]: -3.65,  2.09, -3.88,  3.90, \n",
            "s[3,5]:  1.76,  1.22,  0.52, -0.91, \n",
            "s[3,6]: -6.37,  1.71,  2.39,  3.56, \n",
            "s[3,7]:  4.80, -4.38, -5.24,  1.81, \n",
            "s[3,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,1]:  4.75, -9.07,  5.88, -9.09, \n",
            "s[4,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,3]:  4.33, -4.57,  7.37, -7.73, \n",
            "s[4,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,5]:  1.73, -0.11,  4.33, -5.82, \n",
            "s[4,6]:  1.41, -1.79, -0.68,  2.22, \n",
            "s[4,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,1]:  5.36,  6.53,  5.43, -8.97, \n",
            "s[5,2]: -8.97,  7.26, -8.84,  5.96, \n",
            "s[5,3]:  6.49,  8.07, -9.01,  6.47, \n",
            "s[5,4]: -9.25,  8.97,  5.20,  6.82, \n",
            "s[5,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,6]: -0.54,  0.72,  0.98,  0.08, \n",
            "s[5,7]: -0.51, -0.22, -1.00,  1.20, \n",
            "s[5,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,1]:  5.91, -3.47,  3.93, -7.60, \n",
            "s[6,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,4]:  7.92, -4.25,  1.09, -5.52, \n",
            "s[6,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,6]:  0.38, -0.10, -0.72,  1.00, \n",
            "s[6,7]: -0.47, -0.84, -0.74,  1.10, \n",
            "s[6,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[7,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[7,1]:  4.95,  0.13, -0.87, -2.58, \n",
            "s[7,2]: -1.37,  0.92, -2.58,  2.63, \n",
            "s[7,3]: -0.88,  0.56, -1.52,  1.79, \n",
            "s[7,4]:  0.91,  0.38, -1.71,  1.24, \n",
            "s[7,5]: -1.01,  0.09, -0.72,  1.27, \n",
            "s[7,6]:  1.27, -0.68, -0.20, -0.71, \n",
            "s[7,7]:  0.78, -1.29, -0.34,  0.57, \n",
            "s[7,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,1]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,8]:  0.00,  0.00,  0.00,  0.00, \n"
          ]
        }
      ]
    }
  ]
}
