{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAU9qqeaD7iW"
      },
      "source": [
        "인공지능 과제5 강화학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h2w1-Eb8-vZ6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "#DQN를 위한 라이브러리\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ECphitxI_LH7"
      },
      "outputs": [],
      "source": [
        "total_episodes = 50000       # Total episodes\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.9                 # Discounting rate\n",
        "alpha = 1.0\t\t\t\t\t # update parameter\n",
        "\n",
        "# Exploration parameters\n",
        "original_epsilon = 0.4           # Exploration rate\n",
        "decay_rate = 0.000016            # Exponential decay rate for exploration prob\n",
        "random.seed(datetime.now().timestamp())\t# give a new seed in random number generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QydmbWWy_LKf"
      },
      "outputs": [],
      "source": [
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "\n",
        "max_row = 6\n",
        "max_col = 6\n",
        "max_num_actions = 4\n",
        "\n",
        "env_state_space = \\\n",
        "  [ ['H', 'H', 'H', 'H', 'H', 'H'], \\\n",
        "    ['H', 'S', 'F', 'F', 'F', 'H'], \\\n",
        "\t\t['H', 'F', 'H', 'F', 'H', 'H'], \\\n",
        "\t\t['H', 'F', 'F', 'F', 'H', 'H'], \\\n",
        "\t\t['H', 'H', 'F', 'F', 'G', 'H'], \\\n",
        "\t\t['H', 'H', 'H', 'H', 'H', 'H'] ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1qklKJje_LNL"
      },
      "outputs": [],
      "source": [
        "# Create our Q table and initialize its value.\n",
        "#   dim0:row, dim1:column, dim2: action.\n",
        "# Q-table is initialized as 0.0.\n",
        "# for terminal states(H or G), q-a value should be always 0.\n",
        "\n",
        "Q = np.zeros((max_row,\tmax_col,  max_num_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i8T_WdqS_LPw"
      },
      "outputs": [],
      "source": [
        "# offset of each move action:  up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "#        action = up    right  down    left.\n",
        "move_offset = [[-1,0], [0,1],   [1,0],  [0,-1]]\n",
        "move_str =\t   ['up   ', 'right', 'down ', 'left ']\n",
        "\n",
        "def display_Q_table (Q):\n",
        "\tprint(\"\\ncol=0       1        2        3       4         5\")\n",
        "\tfor r in range(max_row):\n",
        "\t\tprint(\"row:\", r)\n",
        "\t\tfor a in range(max_num_actions):\n",
        "\t\t\tfor c in range(max_col):\n",
        "\t\t\t\ttext = \"{:5.2f}\".format(Q[r,c,a])\n",
        "\t\t\t\tif c == 0:\n",
        "\t\t\t\t\tline = text\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tline = line + \",   \" + text\n",
        "\t\t\tprint(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LyJnNvltL3gI"
      },
      "outputs": [],
      "source": [
        "def my_argmax(q_a_list):\n",
        "    max_value = np.max(q_a_list)\n",
        "    m_positions = np.where(q_a_list == max_value)[0]\n",
        "    nmax = len(m_positions)\n",
        "\n",
        "    # Generate thresholds\n",
        "    thresh = np.arange(1, nmax + 1) / nmax\n",
        "\n",
        "    # Generate a random number\n",
        "    r_n = random.random()\n",
        "\n",
        "    # Find the first position where r_n is less than or equal to the threshold\n",
        "    j = np.argmax(r_n <= thresh)\n",
        "\n",
        "    return m_positions[j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x64RRAyn_LSZ"
      },
      "outputs": [],
      "source": [
        "# choose an action with epsilon-greedy approach according to Q.\n",
        "# return value: an action index(0 ~ 3)\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = my_argmax(q_a_list)  # Use my_argmax instead of np.argmax\n",
        "    rn = random.random()\n",
        "    if rn >= epsilon:\n",
        "        action = max_action\n",
        "    else:\n",
        "        rn1 = random.random()\n",
        "        if rn1 >= 0.75:\n",
        "            action = 0\n",
        "        elif rn1 >= 0.5:\n",
        "            action = 1\n",
        "        elif rn1 >= 0.25:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6ym5UW2J_LVC"
      },
      "outputs": [],
      "source": [
        "# Q 가 가진 policy 를 greedy 하게 적용하여 취할 action 을 고른다.\n",
        "def choose_action_with_greedy(s):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = my_argmax(q_a_list)  # Use my_argmax instead of np.argmax\n",
        "    return max_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K8U_VXPz_LX3"
      },
      "outputs": [],
      "source": [
        "# get new state and reward for taking action a at state s.\n",
        "# deterministic movement is taken.\n",
        "# reward is given as: F/S:0;  H:-5;   G:5.\n",
        "def get_new_state_and_reward(s, a):\n",
        "\tnew_state = []\n",
        "\toff_set = move_offset[a]\n",
        "\n",
        "\t#  s + off_set gives the new_state.\n",
        "\tnew_state.append(s[0] + off_set[0])\n",
        "\tnew_state.append(s[1] + off_set[1])\n",
        "\n",
        "\t# compute reward for moving to the new state\n",
        "\tcell = env_state_space[new_state[0]][new_state[1]]\n",
        "\tif cell == 'F':\n",
        "\t\trew = 0\n",
        "\telif cell == 'H':\n",
        "\t\trew = -9\n",
        "\telif cell == 'G':\n",
        "\t\trew = 9\n",
        "\telif cell == 'S':\n",
        "\t\trew = 0\n",
        "\telse:\n",
        "\t\tprint(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "\t\ttime.sleep(1200)\n",
        "\t\treturn [0,0], -20000\n",
        "\treturn new_state, rew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXuz1JHo_Lay",
        "outputId": "5358f9b4-e8c2-4fa5-c649-4c2c17a1a59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Q table is\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 2\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n"
          ]
        }
      ],
      "source": [
        "# Environment 출력: agent 가 있는 곳에는 * 로 표시.\n",
        "# agent 의 현재 위치(즉 current state): s\n",
        "def env_rendering(s):\n",
        "\tfor i in range(0, max_row, 1):\n",
        "\t\tline = ''\n",
        "\t\tfor j in range(0, max_col, 1):\n",
        "\t\t\tline = line + env_state_space[i][j]\n",
        "\t\tif s[0] == i:\n",
        "\t\t\tcol = s[1]\n",
        "\t\t\tline1 = line[:col] + '*' +line[col+1:]\n",
        "\t\telse:\n",
        "\t\t\tline1 = line\n",
        "\t\tprint(line1)\n",
        "  # Learning stage: it iterates for an huge number of episodes\n",
        "print(\"Initial Q table is\")\n",
        "display_Q_table(Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaZRCFf4_LgG",
        "outputId": "20f7d9d7-36c8-4d49-e12f-a347fee40ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Learning starts.\n",
            "\n",
            "episode= 0   epsilon= 0.4\n",
            "episode= 5000   epsilon= 0.36924653855465434\n",
            "episode= 10000   epsilon= 0.3408575155864846\n",
            "episode= 15000   epsilon= 0.3146511444266214\n",
            "episode= 20000   epsilon= 0.2904596148294764\n",
            "episode= 25000   epsilon= 0.26812801841425576\n",
            "episode= 30000   epsilon= 0.24751335672245633\n",
            "episode= 35000   epsilon= 0.22848362553952595\n",
            "episode= 40000   epsilon= 0.21091696961721942\n",
            "episode= 45000   epsilon= 0.19470090238398868\n",
            "\n",
            "Learning is finished. the Q table is:\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,   -9.00,   -9.00,   -9.00,   -9.00,    0.00\n",
            " 0.00,    5.31,    5.90,    5.31,   -9.00,    0.00\n",
            " 0.00,    5.31,   -9.00,    6.56,   -9.00,    0.00\n",
            " 0.00,   -9.00,    4.78,    5.31,    5.90,    0.00\n",
            "row: 2\n",
            " 0.00,    4.78,    0.00,    5.90,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,    0.00\n",
            " 0.00,    5.90,    0.00,    7.29,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    5.31,   -9.00,    6.56,    0.00,    0.00\n",
            " 0.00,    6.56,    7.29,   -9.00,    0.00,    0.00\n",
            " 0.00,   -9.00,    7.29,    8.10,    0.00,    0.00\n",
            " 0.00,   -9.00,    5.90,    6.56,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    6.56,    7.29,    0.00,    0.00\n",
            " 0.00,    0.00,    8.10,    9.00,    0.00,    0.00\n",
            " 0.00,    0.00,   -9.00,   -9.00,    0.00,    0.00\n",
            " 0.00,    0.00,   -9.00,    7.29,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n"
          ]
        }
      ],
      "source": [
        "# start state is the cell with 'S'. terminal states are those with 'H' or 'G.\n",
        "start_state = [1,1]\n",
        "\n",
        "print(\"\\nLearning starts.\\n\")\n",
        "for episode in range(total_episodes):\n",
        "\t# set the start state of an episode.\n",
        "\tS = start_state\n",
        "\n",
        "\t# we use decayed epsilon in exploration so that it decreases as time goes on.\n",
        "\tepsilon = original_epsilon * math.exp(-decay_rate*episode)\n",
        "\n",
        "\tif episode % 5000 == 0:\n",
        "\t\tprint('episode=', episode, '  epsilon=', epsilon)\n",
        "\t\ttime.sleep(1)\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\t\t# Choose an action A from S using policy derived from Q with epsilon-greedy.\n",
        "\t\tA = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "\n",
        "\t\t# take action A to observe reward R, and new state S_.\n",
        "\t\tS_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "\t\tr=S[0]\t# row of S\n",
        "\t\tc=S[1]\t# column of S\n",
        "\n",
        "\t\t# update state-action value Q(s,a)\n",
        "\t\tQ[r][c][A] = Q[r][c][A] + alpha * (R + gamma * np.max(Q[S_[0]][S_[1]][:]) - Q[r][c][A] )\n",
        "\n",
        "\t\tS = S_\t# move to the new state.\n",
        "\n",
        "\t\t# if the new state S is a terminal state, terminate the episode.\n",
        "\t\tif env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "\t\t\tbreak\n",
        "\n",
        "print('\\nLearning is finished. the Q table is:')\n",
        "display_Q_table(Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHtSA87Z_rml",
        "outputId": "72728d0e-f958-4e6b-f464-10b65b4be3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test starts.\n",
            "\n",
            "\n",
            "Episode: 0  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 1  start state: ( 1 ,  1 )\n",
            "The move is: down   that leads to state ( 2 ,  1 )\n",
            "The move is: down   that leads to state ( 3 ,  1 )\n",
            "The move is: right  that leads to state ( 3 ,  2 )\n",
            "The move is: down   that leads to state ( 4 ,  2 )\n",
            "The move is: right  that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 2  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 3  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 4  start state: ( 1 ,  1 )\n",
            "The move is: down   that leads to state ( 2 ,  1 )\n",
            "The move is: down   that leads to state ( 3 ,  1 )\n",
            "The move is: right  that leads to state ( 3 ,  2 )\n",
            "The move is: right  that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "Program ends!!!\n"
          ]
        }
      ],
      "source": [
        "# time.sleep(600)\n",
        "\n",
        "# Test stage: agent 가 길을 찾아 가는 실험.\n",
        "\n",
        "print(\"\\nTest starts.\\n\")\n",
        "\n",
        "for e in range(5):\n",
        "\tS = start_state\n",
        "\ttotal_rewards = 0\n",
        "\tprint(\"\\nEpisode:\", e, \" start state: (\", S[0], \", \", S[1], \")\")\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\tA = choose_action_with_greedy(S)\t\t# S 에서 greedy 하게 다음 action 을 선택.\n",
        "\n",
        "\t\tS_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "\t\tprint(\"The move is:\", move_str[A], \" that leads to state (\", S_[0], \", \", S_[1], \")\" )\n",
        "\n",
        "\t\ttotal_rewards += R\n",
        "\t\tS = S_\n",
        "\t\tif env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "\n",
        "\t\t\tbreak\n",
        "\tprint(\"Episode has ended. Total reward received in episode = \", total_rewards)\n",
        "\ttime.sleep(1)\n",
        "\n",
        "print(\"Program ends!!!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
