{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rJQDnh1auT78"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1BxTTiWXpHl",
        "outputId": "1d1fd52f-6566-44f4-cdc8-85d1f384edb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## 상수 선언\n",
        "d_embed = 100     ## word embedding vector 의 길이 (원소수)\n",
        "Max_seq_length = 128    ## time step 수 (MSL)\n",
        "Vocab_size = 51459\t# 사전 Vocab 의 총 단어수\n",
        "Num_POS = 50    ## 총 품사 수\n",
        "LEARNING_RATE = 0.7e-4\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NkzhBwHDXat-"
      },
      "outputs": [],
      "source": [
        "# 단어와 그 출현횟수를 dict 타입의 사전에 수집한다.\n",
        "def build_vocabulary_temp(corpus_path):\n",
        "    fp = open(corpus_path, \"r\", encoding=\"utf-8\")\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':\n",
        "            continue\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            ## word/pos 내에 슬래시가 2개 이상 있어 3 조각 이상이 나옴.\n",
        "            if nseg > 2:    ## 마지막 슬래시를 기준으로 단어와 품사로 구분함.\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1]    # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        "\n",
        "            pos = w_p[-1]   # the last segment\n",
        "            if word in Vocab_temporary:\n",
        "                Vocab_temporary[word] += 1\n",
        "            else:\n",
        "                Vocab_temporary[word] = 1\n",
        "    fp.close()\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6LbZtmzXrUI",
        "outputId": "82c86820-19fc-4668-e25a-23e2d83f4549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "파일에서 모은 총 단어수: 51457\n",
            "최종 사전의 총 단어수: 51459\n",
            "총 품사수:  50\n"
          ]
        }
      ],
      "source": [
        "#1 차적인 임시사전을 만든다. 사전명: Vocab_temporary\n",
        "#    key: 단어, value:출현횟수\n",
        "# 수집할 단어들의 대상은 penn-tree-bank 내의 모든 파일에 존재하는 단어들이다!!\n",
        "\n",
        "Vocab_temporary = {}\n",
        "build_vocabulary_temp(\"/content/drive/MyDrive/인공지능실습/all_word_pos_sentences_all.txt\")\n",
        "\n",
        "# 사전 단어들을 출현횟수로 내림차순으로 정렬하여 그 결과를 리스트로 받는다.\n",
        "sorted_Vocab = sorted(Vocab_temporary.items(), key = lambda kv: kv[1], reverse=True)\n",
        "Total_n_words = len(sorted_Vocab)\n",
        "print('파일에서 모은 총 단어수:', Total_n_words)\n",
        "\n",
        "# 두 개의 특수단어('[PAD]', '[UNK]')를 포함하는 정식 사전(사전명: Vocab) 을 만든다:\n",
        "#     key: 단어, value: 단어번호\n",
        "Vocab = {}\n",
        "Vocab['[PAD]'] = 0  # 특수단어 추가\n",
        "Vocab['[UNK]'] = 1  # 특수단어 추가\n",
        "# Vocab_temporary 에 모은 단어들에게는 단어번호를 2 부터 준다.\n",
        "for i in range(Total_n_words):\n",
        "    word = sorted_Vocab[i][0]\n",
        "    freq = sorted_Vocab[i][1]\n",
        "    Vocab[word] = i + 2\n",
        "\n",
        "Total_number_words = len(Vocab)\n",
        "print(\"최종 사전의 총 단어수:\", Total_number_words)\n",
        "\n",
        "## 역-단어사전 i_Vocab  만들기:  Vocab 에서 key와 value 를 바꾼 사전.\n",
        "#    key: 단어번호,  value: 단어\n",
        "all_wps = list(Vocab.keys())\n",
        "i_Vocab = {}\n",
        "for word in all_wps:\n",
        "  widx = Vocab[word]\n",
        "  i_Vocab[widx] = word\n",
        "\n",
        "## 품사 사전 (사전명: dic_POS)만들기:  key:품사명,  value: 품사번호\n",
        "#    두 특수단어에게는 품사명/품사번호를  '[PAD]': 0,     '[UNK]': 1 로 준다.\n",
        "#    나머지는 penn-tree-bank 의 48 개의 품사들에게 품사 번호를 2 부터 부여한다(2~49)\n",
        "#    결국 총 품사는 총 50 개로 번호는 0 ~ 49 가 된다.\n",
        "all_pos_list = ['[PAD]', '[UNK]',\n",
        "                'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS',\n",
        "                'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB',\n",
        "                'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN',\n",
        "                'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '#', '$', '.', ',',\n",
        "                ':', '(', ')', '\\'\\'', '\\'', '``', '&rsquo', '”']\n",
        "dic_POS = {}\n",
        "for i in range(len(all_pos_list)):\n",
        "    dic_POS[all_pos_list[i]] = i\n",
        "\n",
        "num_pos = len(dic_POS)\n",
        "print(\"총 품사수: \", num_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C6XTbsqbhzh1"
      },
      "outputs": [],
      "source": [
        "# 역-품사사전 만들기: dic_POS 에서 key와 value 를 바꾼 사전.\n",
        "#     key: 품사번호,     value: 품사명\n",
        "all_pos = list(dic_POS.keys())\n",
        "i_dic_POS = {}\n",
        "for a_pos in all_pos:\n",
        "  pidx = dic_POS[a_pos]\n",
        "  i_dic_POS[pidx] = a_pos\n",
        "\n",
        "def build_index_sentences(path_word_pos_sentence_file, path_index_sentence_file):\n",
        "    fp = open(path_word_pos_sentence_file, \"r\", encoding=\"utf-8\")\n",
        "    fp_w = open(path_index_sentence_file, \"w\", encoding=\"utf-8\")\n",
        "\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':   ## 파일명을 가지는 줄은 무시한다.\n",
        "            continue\n",
        "\n",
        "        line_widx = ''  # line for word indices\n",
        "        line_pidx = ''  # line for pos indices\n",
        "\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            if nseg > 2:\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1]    # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        "\n",
        "            pos = w_p[-1]   # the last segment\n",
        "            if not(word in Vocab):  # Other scheme: Vocab.get(word) 가 None 이면 없는 것을 맗암.\n",
        "                widx = 1    # give index of [UNK] since it is missing in Vocab.\n",
        "            else:\n",
        "                widx = Vocab[word]\n",
        "\n",
        "            if not(pos in dic_POS):\n",
        "                pos_list = pos.split('|')\n",
        "                pos = pos_list[-1]\n",
        "                if not (pos in dic_POS):\n",
        "                    print(\"exception occurs at dic_POS look_up. w_p=\", w_p, \" pos=\", pos)\n",
        "                    time.sleep(100)\n",
        "                else:\n",
        "                    pidx = dic_POS[pos]\n",
        "            else:\n",
        "                pidx = dic_POS[pos]\n",
        "\n",
        "            if len(line_widx) == 0:\n",
        "                line_widx = line_widx + str(widx)\n",
        "            else:\n",
        "                line_widx = line_widx + '\\t' + str(widx)\n",
        "\n",
        "            if len(line_pidx) == 0:\n",
        "                line_pidx = line_pidx + str(pidx)\n",
        "            else:\n",
        "                line_pidx = line_pidx + '\\t' + str(pidx)\n",
        "\n",
        "        fp_w.write(line_widx + '\\n')\n",
        "        fp_w.write(line_pidx + '\\n')\n",
        "        fp_w.write('\\n')    # an empty line after each sentence\n",
        "    fp_w.close()\n",
        "    fp.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AcOXWGdNh2TI"
      },
      "outputs": [],
      "source": [
        "# 단어/품사 문자열을 이용하는 훈련데이타 파일들로 부터  단어번호/품사번호를 이용하는 훈련데이타 파일들을 만든다.\n",
        "build_index_sentences(\"/content/drive/MyDrive/인공지능실습/all_word_pos_sentences_train.txt\", \"./drive/MyDrive/all_index_sentences_train.txt\")\n",
        "build_index_sentences(\"/content/drive/MyDrive/인공지능실습/all_word_pos_sentences_validation.txt\", \"./drive/MyDrive/all_index_sentences_validation.txt\")\n",
        "build_index_sentences(\"/content/drive/MyDrive/인공지능실습/all_word_pos_sentences_test.txt\", \"./drive/MyDrive/all_index_sentences_test.txt\")\n",
        "\n",
        "\n",
        "##  훈련예제 준비 함수\n",
        "##   파일경로를 입력으로 받는다. 이 파일은 all_index_sentences_???.txt 이다.\n",
        "##  출력: 파일내의 모든 문장들에 대한 정보를 이용하여 다음을 준비하여 출력한다.\n",
        "##       1) list_X: 문장들의 단어 번호 리스트를 원소로 가지는 리스트\n",
        "##       2) list_Y: 문장들의 정답품사번호 리스트를 원소로 가지는 리스트\n",
        "##       3) list_leng: 문장들의 길이를 원소로 가지는 리스트\n",
        "\n",
        "def load_X_and_Y(path_index_file):\n",
        "    fp= open(path_index_file, \"r\", encoding=\"utf-8\")\n",
        "    list_X = []\n",
        "    list_Y = []\n",
        "    list_leng = []\n",
        "\n",
        "    while True:\n",
        "        # read two lines\n",
        "        wordline = fp.readline()\n",
        "        line_leng = len(wordline)\n",
        "\n",
        "        if line_leng == 0:\n",
        "            break   # end of file has come.\n",
        "        if line_leng == 1:\n",
        "            continue    # empty line used as sentence delimeter\n",
        "\n",
        "        # The line read just before is a line of word indices.\n",
        "        # The next line should be the corresponding pos index line.\n",
        "        posline = fp.readline()\n",
        "        w_index = wordline.split()\n",
        "        p_index = posline.split()\n",
        "\n",
        "        # X : a list of indices of words in a sentence.\n",
        "        # Y : a list of pos indices of words in the sentence of X.\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        leng = len(w_index)\n",
        "        if leng > Max_seq_length:\n",
        "            leng = Max_seq_length   # truncation is done.\n",
        "\n",
        "        for i in range(leng):\n",
        "            X.append(int(w_index[i]))\n",
        "            Y.append(int(p_index[i]))\n",
        "\n",
        "        # pads are added after sentence\n",
        "        if leng < Max_seq_length:\n",
        "            for i in range(leng, Max_seq_length):\n",
        "                X.append(0)     # word index of '[PAD]' which is 0 is added.\n",
        "                Y.append(0)     # pos index of  '[PAD]' which is 0 is added.\n",
        "\n",
        "        list_X.append(X)\n",
        "        list_Y.append(Y)\n",
        "        list_leng.append(leng)\n",
        "\n",
        "    fp.close()\n",
        "    return list_X, list_Y, list_leng\n",
        "\n",
        "x_train, y_train, leng_train = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_train.txt\")\n",
        "x_train = np.array(x_train, dtype='i')\n",
        "y_train = np.array(y_train, dtype='i')\n",
        "\n",
        "x_validation, y_validation, leng_valiation = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_validation.txt\")\n",
        "x_validation = np.array(x_validation, dtype='i')\n",
        "y_validation = np.array(y_validation, dtype='i')\n",
        "\n",
        "x_test, y_test, leng_test = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_test.txt\")\n",
        "x_test = np.array(x_test, dtype='i')\n",
        "y_test = np.array(y_test, dtype='i')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-3bwjA8h6SX",
        "outputId": "25ff34ce-c9bc-4884-b3ba-dc0f68983de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "844/844 [==============================] - 95s 81ms/step - loss: 2.9248 - acc: 0.1795 - val_loss: 2.7987 - val_acc: 0.1947\n",
            "Epoch 2/20\n",
            "844/844 [==============================] - 42s 50ms/step - loss: 2.7240 - acc: 0.2159 - val_loss: 2.6113 - val_acc: 0.2607\n",
            "Epoch 3/20\n",
            "844/844 [==============================] - 39s 47ms/step - loss: 2.1237 - acc: 0.4145 - val_loss: 1.6758 - val_acc: 0.5283\n",
            "Epoch 4/20\n",
            "844/844 [==============================] - 42s 49ms/step - loss: 1.3031 - acc: 0.6519 - val_loss: 1.0155 - val_acc: 0.7582\n",
            "Epoch 5/20\n",
            "844/844 [==============================] - 40s 47ms/step - loss: 0.7745 - acc: 0.8270 - val_loss: 0.6540 - val_acc: 0.8553\n",
            "Epoch 6/20\n",
            "844/844 [==============================] - 38s 45ms/step - loss: 0.4968 - acc: 0.8929 - val_loss: 0.4717 - val_acc: 0.8938\n",
            "Epoch 7/20\n",
            "844/844 [==============================] - 39s 47ms/step - loss: 0.3498 - acc: 0.9222 - val_loss: 0.3792 - val_acc: 0.9112\n",
            "Epoch 8/20\n",
            "844/844 [==============================] - 39s 46ms/step - loss: 0.2652 - acc: 0.9395 - val_loss: 0.3191 - val_acc: 0.9248\n",
            "Epoch 9/20\n",
            "844/844 [==============================] - 40s 47ms/step - loss: 0.2108 - acc: 0.9500 - val_loss: 0.2807 - val_acc: 0.9316\n",
            "Epoch 10/20\n",
            "844/844 [==============================] - 36s 43ms/step - loss: 0.1739 - acc: 0.9574 - val_loss: 0.2558 - val_acc: 0.9378\n",
            "Epoch 11/20\n",
            "844/844 [==============================] - 38s 45ms/step - loss: 0.1477 - acc: 0.9629 - val_loss: 0.2424 - val_acc: 0.9407\n",
            "Epoch 12/20\n",
            "844/844 [==============================] - 38s 45ms/step - loss: 0.1289 - acc: 0.9668 - val_loss: 0.2313 - val_acc: 0.9430\n",
            "Epoch 13/20\n",
            "844/844 [==============================] - 38s 45ms/step - loss: 0.1148 - acc: 0.9695 - val_loss: 0.2238 - val_acc: 0.9446\n",
            "Epoch 14/20\n",
            "844/844 [==============================] - 40s 47ms/step - loss: 0.1041 - acc: 0.9714 - val_loss: 0.2173 - val_acc: 0.9456\n",
            "Epoch 15/20\n",
            "844/844 [==============================] - 36s 43ms/step - loss: 0.0956 - acc: 0.9729 - val_loss: 0.2139 - val_acc: 0.9468\n",
            "Epoch 16/20\n",
            "844/844 [==============================] - 37s 44ms/step - loss: 0.0890 - acc: 0.9743 - val_loss: 0.2154 - val_acc: 0.9476\n",
            "Epoch 17/20\n",
            "844/844 [==============================] - 38s 45ms/step - loss: 0.0831 - acc: 0.9756 - val_loss: 0.2109 - val_acc: 0.9480\n",
            "Epoch 18/20\n",
            "844/844 [==============================] - 37s 44ms/step - loss: 0.0784 - acc: 0.9766 - val_loss: 0.2147 - val_acc: 0.9478\n",
            "Epoch 19/20\n",
            "844/844 [==============================] - 39s 46ms/step - loss: 0.0744 - acc: 0.9775 - val_loss: 0.2098 - val_acc: 0.9484\n",
            "Epoch 20/20\n",
            "844/844 [==============================] - 38s 45ms/step - loss: 0.0710 - acc: 0.9783 - val_loss: 0.2156 - val_acc: 0.9483\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Model 설계\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "## 층0:  word-embedding 층\n",
        "model.add(tf.keras.layers.Embedding(Vocab_size, d_embed, embeddings_initializer='random_normal', \\\n",
        "\tinput_length=Max_seq_length, mask_zero=True, trainable=True))\t# output shape: (bsz, MSL, d_emb)\n",
        "\n",
        "## 충1: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), input_shape=(Max_seq_length, d_embed)))\n",
        "\t# output shape: (batch_sz, MSL, 512)\n",
        "\n",
        "## 충2: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(Max_seq_length, 512)))\n",
        "\t# output shape: (batch_sz, MSL, 256)\n",
        "\n",
        "## 충3: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(Max_seq_length, 256)))\n",
        "\t# output shape: (batch_sz, MSL, 128)\n",
        "\n",
        "## 층4: NN 층\n",
        "model.add(tf.keras.layers.Dense(units=Num_POS, activation='softmax', use_bias=True))\t# 최종출력층. 각 시간의 각 단어마다 num_POS=50개의 확률이 생성됨.\n",
        "\t# output shape: (batch_sz, MSL, num_POS)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
        "model.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_validation, y_validation), shuffle=True, verbose=1)\n",
        "\n",
        "print(\"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv11lzAhX5TX",
        "outputId": "58211207-9ca2-4c80-de17-76f696fbeaf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240/240 [==============================] - 10s 11ms/step\n",
            "240/240 [==============================] - 4s 18ms/step - loss: 0.1971 - acc: 0.9514\n",
            "Test Accuracy: 0.9514239430427551\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "##### TEST ####################\n",
        "\n",
        "pred = model.predict(x=x_test, verbose=1)   # batch 단위로  수행함 (default = 32)\n",
        "                                            # batch 별 결과를 모아서 전체 결과를 반환함\n",
        "pred_label = tf.math.argmax(pred, axis=2)\n",
        "pred_label = pred_label.numpy()     # this is of 2-dimension (num_test_sents, msl)\n",
        "\n",
        "num_test_sentences = y_test.shape[0]  # y_test has a shape of (num_test_sents, msl)\n",
        "\n",
        "##\n",
        "##   여기에 accuracy 를 코드를 넣는다.\n",
        "##\n",
        "print(\"test accuracy = \", acc)\n",
        "'''\n",
        "\n",
        "pred = model.predict(x=x_test, verbose=1)   # batch 단위로  수행함 (default = 32)\n",
        "                                            # batch 별 결과를 모아서 전체 결과를 반환함\n",
        "pred_label = tf.math.argmax(pred, axis=2)\n",
        "pred_label = pred_label.numpy()     # this is of 2-dimension (num_test_sents, msl)\n",
        "\n",
        "num_test_sentences = y_test.shape[0]  # y_test has a shape of (num_test_sents, msl)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4TqhLqTYuzw",
        "outputId": "3f4895d3-42c2-4eb9-c994-3b894f54a36d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1) Arthur/NNP M./NNP Goldberg/NNP said/VBD he/PRP extended/VBD his/PRP$ unsolicited/JJ tender/NN offer/NN of/IN $/$ 32/CD \n",
            "(2) a/DT share/NN tender/NN offer/NN ,/, or/CC $/$ 154.3/CD million/CD ,/, for/IN Di/NNP Giorgio/NNP Corp./NNP to/TO Nov./NNP 1/CD ./. \n",
            "(3) DIG/NNP Acquisition/NNP Corp./NNP ,/, the/DT New/NNP Jersey/NNP investor/NN 's/POS acquisition/NN vehicle/NN ,/, said/VBD that/IN as/IN of/IN the/DT close/NN of/IN business/NN yesterday/NN ,/, 560,839/NN<CD> shares/NNS had/VBD been/VBN tendered/VBN ./. \n",
            "(4) Including/VBG the/DT stake/NN DIG/NNP already/RB held/VBD ,/, DIG/NNP holds/VBZ a/DT total/NN of/IN about/RB 25/CD %/NN of/IN Di/NNP Giorgio/NNP 's/POS shares/NNS on/IN a/DT fully/RB diluted/VBN basis/NN ./. \n",
            "(5) The/DT offer/NN ,/, which/WDT also/RB includes/VBZ common/JJ and/CC preferred/JJ<VBN> stock/NN purchase/NN rights/NNS ,/, was/VBD to/TO expire/VB last/JJ night/NN at/IN midnight/NN ./. \n",
            "(6) The/DT new/JJ expiration/NN date/NN is/VBZ the/DT date/NN on/IN which/WDT \n",
            "(7) DIG/NNP 's/POS financing/NN<VBG> commitments/NNS ,/, which/WDT total/VBP about/IN<RB> $/$ 240/CD million/CD ,/, are/VBP to/TO expire/VB ./. \n",
            "(8) DIG/NNP is/VBZ a/DT unit/NN of/IN DIG/NNP Holding/NNP Corp./NNP ,/, a/DT unit/NN of/IN Rose/NNP Partners/NNP L.P/NNP ./. \n",
            "(9) Mr./NNP Goldberg/NNP is/VBZ the/DT sole/JJ general/JJ partner/NN in/IN Rose/NNP Partners/NNP ./. \n",
            "(10) In/IN August/NNP ,/, Di/NNP Giorgio/NNP ,/, a/DT San/NNP Francisco/NNP food/NN products/NNS and/CC building/NN materials/NNS marketing/NN and/CC distribution/NN company/NN ,/, rejected/VBD Mr./NNP Goldberg/NNP 's/POS offer/NN as/IN inadequate/JJ ./. \n"
          ]
        }
      ],
      "source": [
        "# 10 문장에 대한 예측 결과 출력\n",
        "for i in range(10):  # 처음 10 문장을 출력\n",
        "    print(f\"({i + 1}) \", end=\"\")\n",
        "    for j in range(leng_test[i]):\n",
        "        word_index = x_test[i, j]\n",
        "        pos_index = pred_label[i, j]\n",
        "        correct_pos_index = y_test[i, j]\n",
        "\n",
        "        word = i_Vocab[word_index]\n",
        "        pos = i_dic_POS[pos_index]\n",
        "        correct_pos = i_dic_POS[correct_pos_index]\n",
        "\n",
        "        if pos == correct_pos:\n",
        "            print(f\"{word}/{pos} \", end=\"\")\n",
        "        else:\n",
        "            print(f\"{word}/{pos}<{correct_pos}> \", end=\"\")\n",
        "\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
